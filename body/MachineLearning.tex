\chapter{机器学习}
这一部分主要是对JDAI中级培训的内容总结，对于课程中重要但又未详细讲解的地方，也会添加一些其他教材中的内容作为补充。

\section{准备知识}
\subsection{概述}
机器学习的基本过程：

1. 监督学习（有已知样本）：信息获取与预处理-->特征提取与特征选择-->分类器设计（训练）-->分类决策（识别）

2. 非监督学习（无已知样本）：信息获取与预处理-->特征提取与特征选择-->聚类（自学习）-->结果解释

机器学习技术主要包括:

1. 分类:决策树、K近邻、逻辑回归、支持向量机、神经网络等。

2. 聚类：C均值、模糊C均值、改进的模糊C均值等。

机器学习的一些基本名词：
\begin{enumerate}
\item 特征(Feature)：样本的任何可区分且可观测的方面。包括定量、定性特征，但通常都转化为定量特征。特征向量、特征空间。
\item 分类器：能够将每一个样本都分到某个类别中（或者拒绝）的算法。
\end{enumerate}

\subsection{统计学习三要素}
统计学习方法=模型+策略+算法。以下对监督学习进行阐述。

{\hei{模型}}就是所要学习到的条件概率分布或是决策函数。假设空间(hypothesis space)就是所有可能模型组成的集合。
决策函数表示的模型为非概率模型，条件概率分布表示的模型为概率模型。

{\hei{策略}}可以理解为确定优化目标函数，即按照什么样的准则来学习/选择最优模型。
损失函数度量模型一次预测的好坏，风险函数度量平均意义下的模型预测好坏。学习的目标是选择期望风险最小的模型。

训练数据集上的平均损失为经验风险。经验风险最小化、结构风险最小化。结构风险最小化等价于正则化。

{\color{red} 损失函数、代价函数、目标函数的区别}
\begin{enumerate}
\item 损失函数(loss fun.)，也可以理解为误差函数(error fun.)，针对单个样本点。
\item 代价函数(cost fun.)，针对整个样本集/训练集而言。可以认为是所有样本点损失函数的平均。
\item 目标函数(object fun.)，一般是代价函数+正则项。    
\end{enumerate}

{\hei{算法}}是指学习模型的具体计算方法。


\subsection{模型评估}
模型评估的必要性？

{\hei{$\blacksquare$ 过拟合问题}}

Mitchell在Machine Learning中给了一个定义：Given a hypothesis space $H$, a hypothesis $h \in H$ 
is said to overfit the training data if there exists some alternative hypothesis $h' \in H$,
such that $h$ has smaller error than $h'$ over the training examples, but $h'$ has a smaller
error than $h$ over the entire distribution of instances.

在这个定义里并没有严格证明一定存在$h'$，所以可以理解过拟合也是一个相对而言的问题。
简单来说，过拟合指所选取的模型在已知数据集上的预测效果很好，但对未知数据预测效果很差的现象。
过拟合本质是一味追求在已知数据上的预测能力，选取的模型复杂度往往比真实模型更高，使得将样本数据本身所包含的噪声也作为真实数据进行学习。

什么是模型复杂度？
如何解决过拟合问题？

{\hei{$\blacksquare$ 混淆矩阵}}

针对实际问题时，主要关注混淆矩阵中非对角线上的元素，也就是分类器判断错误的样本。

多分类问题也可以生成混淆矩阵，只不过不再是$2 \times 2$的矩阵(类别True、False),而是类别数的$n \times n$的矩阵。

{\hei{$\blacksquare$ ROC曲线}}

坐标横轴为False Positive Rate(FPR, FP/P)，坐标纵轴为True Positive Rate(TPR, TP/{\color{red}N})


\subsection{归一化与标准化}
待确定

1. 归一化对原始数据每一个特征的缩放，使得每个特征的联合分布的情况被改变（椭圆变成圆）;

2. 但是由于各个特征的重要性平衡，在迭代求解的时候速度更快;

3. 标准化也是对各个特征进行缩放，但保持原联合分布,只是使得各特征之间能进行比较。

\subsection{极大似然估计MLE与最大后验估计MAP}
参考：https://zhuanlan.zhihu.com/p/32480810

\subsection{交叉熵代价函数}
{\hei{$\blacksquare$ 信息熵}}： 消除不确定性所需要的信息量的度量。

令事件x的信息量表示为$I(x)$,假设信息量与概率有关。
考虑两个独立不相关事件x和y，显然有$I(x,y) = I(x)+I(y)$，而$p(x,y)=p(x)*p(y)$。
所以可以定义信息量为：
\begin{equation*}
    I(x) = -\log p(x)
\end{equation*}
取负号是为了保证信息量为正。对数函数可以任意选取，信息论中以2为底，机器学习通常以e为底。
对于随机变量X，信息熵为：
\begin{equation*}
    H(X) = \mathbb{E}[-\log p(X)] = - \sum_i p(x_i) \log p(x_i)
\end{equation*}

{\hei{$\blacksquare$ 相对熵(KL散度)}}：描述两个概率分布的非对称性度量。定义为
\begin{equation*}
    D_{KL}(p||q) = -\sum_i p(x_i) \log \frac{q(x_i)}{p(x_i)}
\end{equation*}
{\color{red}尽管从直觉上KL散度是个度量或距离函数, 但是它实际上并不是一个真正的度量或距離。因為KL散度不具有对称性：从分布P到Q的距离通常并不等于从Q到P的距离。
$D_{KL}(p||q) \neq D_{KL}(q||p)$} 

通常$p(x)$表示真实（样本）概率分布，$q(x)$表示模型概率分布。显然可以使用相对熵作为代价函数，使得模型生成的概率分布逼近真实样本的概率分布。
所以进一步，对$D_{KL}(p||q)$变换可得
\begin{align*}
    D_{KL}(p||q) &= -\sum_i p(x_i) \log q(x_i) + \sum_i p(x_i) \log p(x_i) \\
    &= H(p,q) - H(p)
\end{align*}
其中$H(p,q) = -\sum_i p(x_i) \log q(x_i)$即表示交叉熵。
由于真实（样本）概率分布是固定的，所以交叉熵也可以作为代价函数，计算更简便。

对于二分类问题，单个样本点上的损失函数可以表示为二元交叉熵的形式：
\begin{equation*}
    L = -[y\log \hat{y} + (1-y)\log (1-\hat{y})]
\end{equation*}

\subsection{对数损失函数}
对数损失函数为
\begin{equation*}
    L(Y, P(Y|X)) = -\log P(Y|X)
\end{equation*}

\section{梯度下降及线性回归}
\subsection{基本算法}
求解无约束最优化问题：
\begin{equation*}
    \min_{x \in R^n} f(x)
\end{equation*}
从初始值$x^{(0)}$开始，按照负梯度方向逐步迭代直到收敛。令$g_k = \nabla f(x^{(k)})$表示$f(x)$在$x^{(k)}$处的梯度，
则迭代公式为：
\begin{equation*}
    x^{(k+1)} = x^{(k)} - \alpha_k g_k
\end{equation*}
当$||g_k|| < \epsilon$、$||f(x^{(k+1)}) - f(x^{(k)})|| < \epsilon$或者$||x^{(k+1)} - x^{(k)} < \epsilon||$时停止迭代。

需要注意的是关于学习率或者步长$\alpha$，通常选取为固定值，但是在《统计学习方法》的附录中，$\alpha$是通过一维搜索确定的：
$\alpha_k = \arg \min f(x^{(k)} - \alpha g_k)$

\subsection{线性回归}
假设共$m$个样本点，每个样本具有$n$维特征，则定义$X \in R^{m \times (n+1)}$，$Y \in R^{m\times 1}$，
$\theta \in R^{(n+1)\times 1}$，其中$X$包含了常值特征，$\theta$包含了偏置系数。
\begin{align*}
    L(\theta) &= \frac{1}{m} (Y-X\theta)^T (Y-X\theta) \\
    &= \frac{1}{m} \sum_{i=1}^m (y_i - x_i \theta)^2
\end{align*}
梯度为：
\begin{equation*}
    \nabla_\theta L(\theta) = -\frac{2}{m} X^T (Y-X\theta)
\end{equation*}

{\hei{$\bigstar$ 思考与总结}}
\begin{enumerate}
\item 每一个样本点按照行向量的形式表示，这可以从代码的角度理解：新数据都是先填充完一行，在堆叠。例如二维vector。
\item 可以预先增加对样本的标准化/归一化处理，这样求解的步长、收敛阈值等都可以固定，同时可以显著提高收敛速度。
\end{enumerate}


\section{Logistic Regression, LR}
二分类LR模型为：
\begin{gather*}
    P(y=1|x) = \frac{e^{w\cdot x + b}}{1 + e^{w\cdot x + b}}\\
    P(y=0|x) = \frac{1}{1 + e^{w\cdot x + b}}
\end{gather*}
其中$y\in{0,1}$为输出。

假设一个事件的发生概率为$p$，定义事件发生的几率(odds)为发生概率与不发生概率的比值，则对数几率(log odds)或logit函数：
\begin{equation*}
    \text{logit} = \log \frac{p}{1-p}
\end{equation*}
那么对LR而言，有：
\begin{equation*}
    \text{logit}(x) = \log\frac{P(y=1|x)}{1-P({y=1|x})} = w\cdot x + b
\end{equation*}
也就是说线性运算的结果对应表示该事件$A(y=1|x)$的对数几率。

可以通过比较$P(y=1|x)$和$P(y=0|x)$的大小，将$x$进行分类。但logistic决策的本质是根据logit(x)判断：
若$\text{logit}(x) > 0$，则$x\in y_1$;若$\text{logit}(x) < 0$，则$x\in y_0$

可以将二分类LR模型推广到多分类LR模型，其实就是Softmax回归：
% \begin{gather*}
%     P(y=k|x) = \frac{\text{exp}(w_k)\cdot x}{1 + \sum_{k=1}^{K-1} \text{exp}(w_k \cdot x)},\, k=1,2,\cdots,K-1 \\
%     P(y=K|x) = \frac{1}{1 + \sum_{k=1}^{K-1} \text{exp}(w_k \cdot x)}
% \end{gather*}
\begin{equation*}
    P(y=k|x) = \frac{\text{exp}(w_k\cdot x + b_k)}{\sum_{j=1}^K \text{exp}(w_j \cdot x + b_j)}
\end{equation*}

\subsection{Sigmoid函数与Softmax函数的区别}
Sigmoid函数形式为：
\begin{equation*}
    \sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^{x}}{1 + e^{x}}
\end{equation*}

Softmax函数形式为：
\begin{equation*}
    \textbf{Softmax}(x_j) = \frac{e^{x_j}}{\sum_{k=1}^K e^{x_k}},\, j=1,2,\cdots,K
\end{equation*}

Sigmoid函数只考虑单变量，将其映射到(0,1)区间内，即使对向量做运算，向量每个元素间的结果无联系。
Softmax函数则考虑了向量中的每一个元素，虽然每个元素的值也是映射到(0,1)区间内，但各元素之和为1。

对于二值分类问题，Sigmoid函数和Softmax函数都输出两个值,假设输出[0, 1]代表“是”，输出[1, 0]代表“否”。\\
那么Softmax可能输出[0.3, 0.7]，代表算法认为“是”的概率是0.7，“否”的概率是0.3，相加为1。\\
Sigmoid的输出可能是[0.4, 0.8]，它们相加不为1。
解释来说就是Sigmoid认为输出第一位为1的概率是0.4，第一位不为1的概率是0.6（1-p），第二位为1的概率是0.8，第二位不为1的概率是0.2。

\subsection{Sigmoid函数多分类与Softmax函数多分类的区别}
按照是否属于该类别的思路，可以设计多个使用Sigmoid函数的LR分类器。但是这样对每个分类器训练的时候，是/不是该类别的数据样本量会出现偏差。
输出的结果向量中，每一个元素表示是/不是情况下，是该类别的概率，每个元素间不相关。

而使用Softmax多分类，则输出结果表示每一个类别的可能性。

\subsection{二分类逻辑回归求解}
从极大似然估计推导。二分类逻辑回归模型可以统一表示为：
\begin{equation*}
    P(y|x;\theta) = (\sigma_\theta(x))^y \cdot (1-\sigma_\theta(x))^{(1-y)}
\end{equation*}
其中，$\sigma_\theta(x) = \sigma(\theta^T x)$。
二分类逻辑回归就是在样本$X$上找到最大可能的概率分布$P(Y|X;\theta)$，显然是一个求解极大似然的问题。
似然函数：
\begin{align*}
    L(\theta) &= P(Y|X;\theta) = \prod_i P(y_i|x_i;\theta) 
\end{align*}
取对数并取反,即采用对数损失函数（交叉熵代价函数）
\begin{align*}
    L = -\log L(\theta) &= -\log P(Y|X;\theta) = -\sum_i \log P(y_i|x_i;\theta) \\ 
    &= - \sum_i \left[ y_i \log (\sigma_\theta(x_i)) + (1-y_i) \log (1-\sigma_\theta(x_i))\right]
\end{align*}

由于$\nabla_x \sigma(x) = \sigma(x)(1-\sigma(x))$，所以对$L$求梯度可得：
\begin{equation*}
    \nabla_\theta L = -\sum_i (y_i - \sigma_\theta(x_i))x_i
\end{equation*}

{\hei{$\bigstar$ 为什么逻辑回归要使用交叉熵代价函数？}}

1. 为什么可以使用交叉熵代价函数：目的是为了让样本预测的概率可能性最大，
并且训练求解参数的速度是比较快的，而且更新速度只和x，y有关，比较的稳定。

2. 为什么不用平方损失函数：梯度更新速度和 sigmod 函数的梯度相关，训练速度会非常慢且有梯度消失的可能。
而且平方损失会导致损失函数是 theta 的非凸函数，不利于求解，因为非凸函数存在很多局部最优解。

使用交叉熵代价（对数损失）函数的本质目的在于极大似然的原理。即使激活函数使用ReLU,仍然可以使用。

\subsection{softmax多分类求解}
采用softmax的多分类逻辑回归模型可以统一表示为：
\begin{equation*}
    P(y|x;\theta) = \sum_{i=1}^k \mathbb{1}\{ y_j=i \} \frac{e^{\theta_i^T x}}{\sum_k e^{\theta_k^T x}}
\end{equation*}
同样按照求解极大似然估计问题，取对数损失函数为：
\begin{align*}
    L &= -\log P(Y|X;\theta) = -\sum_j P(y_j|x_j;\theta) \\
    &= -\sum_{j=1}^{m} \sum_{i=1}^k \mathbb{1}\{ y_j=i \} \log \left[\frac{e^{\theta_i^T x_j}}{\sum_k e^{\theta_k^T x_j}} \right] \\
    &= -\sum_{j=1}^{m} \sum_{i=1}^k \mathbb{1}\{ y_j=i \} \left( \theta_i^T x_j - \log \sum_k e^{\theta_k^T x_j} \right)\\
\end{align*}
求梯度可的：
\begin{align*}
    \Q{L(\theta)}{\theta_i} &= -\sum_{j=1}^{m} \mathbb{1}\{ y_j=i \} \left(x_j - \Q{\log \sum_k e^{\theta_k^T x_j}}{\theta_i} \right)\\
    &= -\sum_{j=1}^{m} \mathbb{1}\{ y_j=i \} \left(x_j - x_j \frac{e^{\theta_i^T x_j}}{\sum_k e^{\theta_k^T x_j}} \right)\\
    &= -\sum_{j=1}^{m} x_j \left[ \mathbb{1}\{ y_j=i \} - P(y_j=i|x_j;\theta) \right]
\end{align*}


\section{支持向量机 SVM}
是二分类模型，基本模型是定义在特征空间上间隔最大的线性分类器。使用核技巧的时候，成为非线性非类器。
分离超平面为$x^T w + b = 0$，决策函数：
\begin{equation*}
    f(x) = \text{sign}(x^T w + b)
\end{equation*}

基本的思路：
\begin{enumerate}
\item 定义样本点到超平面$(w,b)$的距离最小值
\item 如何表示每个样本点的距离都大于这个最小值？$y_i(x_i^T w + b) \ge \hat{\gamma}$
\item 优化问题就是最大化这个最小值。
\item 等价为最小化$||w||^2$在给定的约束情况下
\item 求解对偶问题
\end{enumerate}

\subsection{SMO}
拉格朗日对偶
SMO
核函数基本是面试必问了

比如为什么要用拉格朗日对偶，核函数的本质是什么，SMO与坐标下降法的区别

\subsection{优缺点}
{\hei{$\blacksquare$ 优点}}
\begin{enumerate}
\item 在小样本上是一种不同于概率学习的方法，也具有较好的分类准确性
\item 判别函数只和支持向量相关，相当于抓住了关键样本，具有较好的鲁棒性
\end{enumerate}

{\hei{$\blacksquare$ 缺点}}
\begin{enumerate}
\item 样本量较大的时候计算复杂度难以承受
\item 只能用于二分类问题
\end{enumerate}



\subsection{L2正则逻辑回归与SVM的对比}
逻辑回归的L2正则化，与最大后验估计推导

Regulization
上面提到了，结构风险函数包括了经验风险项和正则项，加入正则项相当于对参数加入了一个先验分布，常用的有L1和L2正则，L1，L2正则化项对模型的参数向量进行“惩罚”，从而避免单纯最小二乘问题的过拟合问题。正则化项本质上是一种先验信息，整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式，如果将这个贝叶斯最大后验估计的形式取对数，即进行极大似然估计，你就会发现问题立马变成了损失函数+正则化项的最优化问题形式。
在逻辑回归求解中，如果对样本加上一个先验的服从高斯分布的假设，那么就取log后，式子经过化简后就变成在经验风险项后面加上一个正则项，此时损失函数变为。
参考https://ngunauj.github.io/2017/08/23/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/


参考:https://www.zhihu.com/question/24904422

注意这里逻辑回归中标签值与SVM一样，都是$\pm 1$

todo