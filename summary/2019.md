# Summary
## January
### 2019/01/02

**工作要点**
1. 基于TF相对完整的实现了比较简单的DQN算法。
2. 总结并应用了相对清晰的设计流程。
3. experience replay部分存在问题。

<!-- **DQN算法理解** -->
<!-- 这部分待放入详细记录中。 -->

**神经网络结构设计流程**
1. 先确定神经网络的数据维度，使用tf.placeholder()进行指定。通常每一行为一个样本数据，所以维度一般是(?,n)，n为特征维度。
2. 自定义建立所使用的神经网络结构，这里主要采用了mlp()函数，参考spinning up相关代码。
3. 确定损失函数的形式。即根据RL算法中的梯度、更新形式，设计好相应的损失函数。
4. 确定好数据的获取、存储及神经网络的训练模式。
    - 获取为最基本的交互形式：o, r, d, _ = env.step(act)
    - 需要保存[o, a, r, o_]
    - 可以按照MC的形式，固定模型参数并采集一组episode 或是 完整的epoch 后进行训练
    - 也可以按照论文中experience replay的形式，每步迭代时都从存储空间中随机抽取样本组成minibatch并训练。**但是目前效果似乎有问题**
  
  
**DQN 损失函数的设计**

DQN的损失函数的形式为：
$$L_i(\theta_i) = \mathbb{E}{[y_i - Q(s,a;\theta_i)]^2}$$
其中，yi是样本标签，根据输入状态且固定网络参数后计算得到。
$$ y_i = r + \gamma \arg\max_{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i)$$

Q(s,a)则是每一步指定的(s,a)对应的值函数的值。但是由于还不知道怎么在placeholder不确定行数的情况下抽取每一行的指定列，所以这里计算的方法为：
1. 先计算每一个s对应的所有q_a(s)
2. 将q_a(s)赋值给target,然后根据记录的数据，将target中每一行对应使用a的那一列，改为y_i右边的前两项。


**算法实现流程**
1. 先在jupter里面进行设计，方便追踪和调试
2. 最开始的版本应该是特例版本：环境、参数、网络形式等。
3. 跑通后再对结构进行优化
4. 应该添加模型参数保存和读取部分，避免重复训练。




**ToDo List**
- [ ] 调试experience replay部分
- [ ] 看看DQN在 Nature上的论文 变化了什么地方


