# Summary
## January
### 2019/01/02 WEEK-1

**工作要点**
1. 基于TF相对完整的实现了比较简单的DQN算法。
2. 总结并应用了相对清晰的设计流程。
3. experience replay部分存在问题。

<!-- **DQN算法理解** -->
<!-- 这部分待放入详细记录中。 -->

**神经网络结构设计流程**
1. 先确定神经网络的数据维度，使用tf.placeholder()进行指定。通常每一行为一个样本数据，所以维度一般是(?,n)，n为特征维度。
2. 自定义建立所使用的神经网络结构，这里主要采用了mlp()函数，参考spinning up相关代码。
3. 确定损失函数的形式。即根据RL算法中的梯度、更新形式，设计好相应的损失函数。
4. 确定好数据的获取、存储及神经网络的训练模式。
    - 获取为最基本的交互形式：o, r, d, _ = env.step(act)
    - 需要保存[o, a, r, o_]
    - 可以按照MC的形式，固定模型参数并采集一组episode 或是 完整的epoch 后进行训练
    - 也可以按照论文中experience replay的形式，每步迭代时都从存储空间中随机抽取样本组成minibatch并训练。**但是目前效果似乎有问题**
  
  
**DQN 损失函数的设计**

DQN的损失函数的形式为：
$$L_i(\theta_i) = \mathbb{E}{[y_i - Q(s,a;\theta_i)]^2}$$
其中，yi是样本标签，根据输入状态且固定网络参数后计算得到。
$$ y_i = r + \gamma \arg\max_{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i)$$

Q(s,a)则是每一步指定的(s,a)对应的值函数的值。但是由于还不知道怎么在placeholder不确定行数的情况下抽取每一行的指定列，所以这里计算的方法为：
1. 先计算每一个s对应的所有q_a(s)
2. 将q_a(s)赋值给target,然后根据记录的数据，将target中每一行对应使用a的那一列，改为y_i右边的前两项。


**算法实现流程**
1. 先在jupter里面进行设计，方便追踪和调试
2. 最开始的版本应该是特例版本：环境、参数、网络形式等。
3. 跑通后再对结构进行优化
4. 应该添加模型参数保存和读取部分，避免重复训练。




**ToDo List**
- [ ] 调试experience replay部分
- [X] 看看DQN在 Nature上的论文 变化了什么地方

## 2019-01-03 WEEK-1
**工作要点**
1. 粗读了DQN在Nature上的论文
2. 重新更新了JDAI课程的SVM作业

**Nature上的 DQN 升级版**
1. experience play部分，提到了可以不按照随机抽样的方式，而是可以考虑抽取能学习最多知识的样本，类似prioritized sweeping。但是文章中没有给出明确的实现。
2. 强调使用分离网络，即采用两个相同结构的NN，一个固定网络参数用于计算yi，另一个则从该参数开始迭代更新。当经过一定steps之后，才将新的参数赋值到第一个网络上。
3. 提到将误差项修剪到-1～1之间后能够提高网络的稳定性，但是也没有给出实现方法。

**SVM作业心得**
1. 将训练集的原始图像转变为二值图。
   - 大多数图片可以直接采用otsu，但是有些图片上下曝光量不一样，这个时候会出现较大误差。对于这些图片，可以将原图片分块进行otsu，最后拼接到一起。
   - 但是大多数图片如果也采用分块的方式，结果并不理想，**尤其是分块是纯色的情况**。
   - 目前还没有比较好的统一的处理办法，仍然是将曝光不同的图片手动调整。**猜想**可以采用权重的方式，将两种方法进行组合。
2. 特征组合
   - 单一特征无法实现100%正确率，错误分类的数量如下，随机选取300/500个样本训练无差别
   - 采用特征组合的方式，并且给不同的特征设计权重，特征1权重5,特征3权重1,在500个训练样本下可以做到100%正确分类。

    |特征编号|1|2|3|4|5|6|
    |:---:|:---:|:---:|:---:|:---:|:---:|:---:|
    |错分类数|195|9|1|853|23|65|


## 2019-01-04 WEEK-1
**工作要点**
1. 修改resume，搜索JD


## 2019-01-07 WEEK-2
**工作要点**
1. 学习TRPO代码

**TRPO代码小结**
1. 仍然是在Actor-Critic框架下进行，值函数估计的过程与VPG相同，Policy学习的过程中，不再是直接对目标函数进行简单的梯度下降优化求解了。
2. TRPO的核心思想是解决Policy更新过程中应该选取什么样的步长，或是如何才能做到有效更新。
3. 目标函数是surrogate advantage函数，用来评价更新后的策略相对原策略有多大的提升。
4. 约束函数是更新前后策略的平均KL散度，用来约束参数的变化幅度，确保参数变化较小。一方面保证稳定性，另一方面推导过程中也假设了参数变化很小。
5. 实际计算过程中采样Talyor展开，目标函数一次展开求解梯度，约束函数二次展开。


**其他几点思考**
1. 为什么Q-Learning不需要进行重要性采样？
   知乎上有一个回答，Q-Learning只进行一步更新，更新的时候状态的分布没有改变。

2. VPG在Policy更新过程中，为什么也没有进行重要性采样？
    - VPG是on-policy策略，使用 pi_theta 采样生成episode样本，再利用这些样本优化pi。
    - TRPO本身也是on-policy策略，其计算过程中使用重要性采样，将公式(13)转化为公式(14)

## 2019-01-08 WEEK-2
**工作要点**
1. 梳理TRPO论文和代码
2. 学习相关的一些知识

**采样的几点理解**

[参考博客](https://www.jianshu.com/p/3d30070932a8)

1. 通常先产生均匀分布的样本，再以这些样本为基础产生符合指定分布的样本，也就是**直接采样**
    - 按均匀分布采样z~U(0,1)
    - 假设期望的分布p(x)的累积分布函数（CDF）为h(x)
    - 求解x=h^(-1)*z, x~p(x)
2. 但是如果h(x)未知或是无法求解出来，那么直接采样就无法进行。可以使用
    - 接受-拒绝采样
    - 重要性采样
3. 无论上述那种采样，随机变量的概率密度函数**p(x)都是已知的**，应该是CDF无法求解，所以才会进行处理
4. 接受-拒绝采样能够产生符合p(x)分布的样本。
5. 重要性采样通常用于求期望，但是可以认为p(xi)/q(xi) * f(xi)就是符合p(x)的样本点么？


**TRPO论文的理解**

[参考论文](http://joschu.net/docs/thesis.pdf)

策略更新可以写成如下的形式：
$$\eta(\tilde{\pi}) = \eta(\pi) + \sum_s{\rho_{\tilde{\pi}}(s)} \sum_a{\tilde{\pi}}(a|s)A^{\pi}(s,a)$$

由于 $$\rho_{\tilde{\pi}}(s)$$ 与 $$\tilde{\pi}$$ 的复杂相关性，导致对上式直接优化比较困难。首先进行局部近似:

$$L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s{\rho_{{\pi}}(s)} \sum_a{\tilde{\pi}}(a|s)A^{\pi}(s,a)$$

在原策略\theta_0处，$$L$$ 与 $$\eta$$ 具有相同取值和相同梯度。这说明：在一个足够小的步长下，对L的提升也能对\eta产生提升。所以可以**用L来作为代理项**执行后续算法。

理论推导可以得到,在$$\alpha << 1 $$时：
$$\eta(\pi_{new}) \ge L_{\pi_{old}} - \frac{2\epsilon\gamma}{(1-\gamma)^2}\alpha^2$$
$$where \ \ \epsilon = \max_s{\mathbb{E}_{a\sim\pi'(a|s)}[A^{\pi}(s,a)]}$$
但是上述结论是对一种混合策略给出的。当推广到广义随机策略的时候，用$$\alpha$$表示新旧策略的一种距离度量，按下式取值的时候上式仍成立：
$$\alpha = D_{TV}^{max}(\pi_{old},\pi_{new}) = \max_s D_{TV}(\pi_{old}(\cdot|s)||\pi_{new}(\cdot|s))$$

由于：
$$D_{TV}(p||q)^2 \le D_{KL}(p||q)$$
所以可以得到：
$$\eta(\tilde{\pi}) \le L_{\pi}(\tilde{\pi}) - CD_{KL}^{max}(\pi, \tilde{\pi})$$
$$where \ \, C=\frac{2\epsilon\gamma}{(1-\gamma)^2}$$

基于该不等式，可以得到一个保证gamma期望非递减的近似策略迭代算法。而Trust region policy optimization(TRPO)则是对该算法的近似，将原算法中惩罚项 KL散度 作为约束进行处理。

原问题：
$$\max_{\theta}[L_{\theta_{old}}(\theta) - CD_{KL}^{max}(\theta_{old}, \theta)]$$
用平均KL散度做近似，则转化为：
$$\max_{\theta}L_{\theta_{old}(\theta)}$$
$$s.t. \ \  \bar{D}_{KL}^{\rho_{\theta_{old}}}(\theta_{old}, \theta) \le \delta$$

展开$$L_{\theta_{old}}$$，目标函数可以变化为：
$$\max_{\theta} \sum_s{\rho_{\theta_{old}}(s)}\sum_a{\pi_{\theta}(a|s)A_{\theta_{old}}(s,a)}$$

替换,**为什么？**
$$\sum_s{\rho_{\theta_{old}}(s)[\cdots]} = \frac{1}{1-\gamma} \mathbb{E}_{s\sim\rho_{\theta_{old}}}[\cdots]$$
并使用重要性采样，将对a的求和转化为在q概率分布下的期望，最后优化问题转化为：
$$\max_{\theta}{\mathbb{E}_{s\sim\rho_{\theta_{old}},a\sim q}\left[\frac{\pi_{\theta}(a|s)}{q(a|s)}Q_{\theta_{old}}(s,a)\right]}$$
$$ s.t. \ \  \mathbb{E}_{s\sim\rho_{\theta_{old}}}\left[D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))\right] \le \delta $$

关于该优化问题的近似有效求解：
1. 计算搜索方向：对目标函数进行线性逼近，对约束进行二次逼近;使用共轭梯度方法求解 Ax=g
2. 在搜索方向上进行线性搜索。

$$\bar{D}_{KL}(\theta_{old}, \theta) \approx \frac{1}{2}(\theta-\theta_{old})^T A (\theta - \theta_{old})  $$

假设搜索方向解得 s = A^{-1}g，搜索步长为\beta：
$$\delta = \bar{D}_{KL} \approx \frac{1}{2}(\beta s)^T A  (\beta s) = \frac{1}{2}\beta^2 s^T A s$$
$$\beta = \sqrt{\frac{2\delta}{s^T A s}} = \sqrt{\frac{2\delta}{g^T A^{-1} g}} $$

在TRPO代码中，引入的线性搜索为：
$$\theta = \theta_{old} + \alpha^j \beta s $$
即求满足约束最小j，\alpha 在(0,1)之间

***最后Critic部分是对值函数V(s)的估计***

其他相关讲解：
[知乎专栏](https://zhuanlan.zhihu.com/p/33704986)


## 2019-01-09 WEEK-2
**工作要点**
1. 学习PPO论文和代码
2. 学习DPG、DDPG论文
3. 学习相关的一些知识

**PPO算法小结**

这里主要讨论的是PPO-Clip。在PPO代码中采用的是简化版的PPO-Clip目标函数：
$$L_{\theta_k}^{CLIP}(\theta) \doteq \mathbb{E}_{s,a\sim \theta_k} \left[ \min\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k} (a|s)} A^{\theta_k}(s,a), \text{clip}\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k} (a|s)}, 1-\epsilon, 1+\epsilon \right) A^{\theta_k}(s,a) \right) \right]$$
clip是截断函数，所以上述目标函数的本质就是做了一个饱和边界处理，若令r表示重要性采样比率，那么就相当于将r约束在1-\epsilon和1+\epsilon范围内。

在PPO-Clip原始论文中，采用的目标函数是：
$$L_t^{\text{CLIP+VF+S}}(\theta) = \mathbb{\hat{E}}_t [L_t^{\text{CLIP}}(\theta) - c_1 L_t^{\text{VF}}(\theta) + c_2 S[\pi_\theta](s_t)] $$
目的是在对策略更新和值函数估计的网络中共享参数，所以设计了一个融合目标函数。其中等号右边第三项是熵奖励，用于提供足够的探索度。

**DDPG算法小结**

DDPG仍然采用Actor-Critic框架。Actor部分使用DPG思想，根据状态产生确定性策略;Critic部分使用DQN思想，对价值-动作值函数进行估计。

***Actor***

在策略梯度里，最初始的目标函数为：
$$ J(\pi) = \mathbb{E}[r_1^\gamma|\pi] = \sum_{t=1}^{\infty}\gamma^{t-1}r(s_t, a_t)$$ 


论文使用的目标函数有些不同，概率性策略为：
$$ J(\theta) = \int_{\mathcal{S}}\rho^{\pi}(\theta) \int_{\mathcal{A}} \pi_{\theta}(s,a)r(s,a) dads = \mathbb{E}_{s\sim\rho^{\pi},a\sim\pi_{\theta}}[r(s,a)]  $$
注意：
$$ \rho^{\pi}(s') \doteq \int_{\mathcal{S}} \sum_{t=1}^{\infty} \gamma^{t-1} p_1(s) p(s \rightarrow s', t, \pi) ds $$
表示折扣状态分布，其中**右端第三项表示：从s开始，在策略pi作用下，经过t步转移后，到达s‘的概率**。

确定性策略下：
$$ J(\mu_{\theta}) = \int_{\mathcal{S}}\rho^\mu(s)r(s,\mu_{\theta}(s))ds = \mathbb{E}_{s\sim\rho^\mu}[r(s,\mu_{\theta}(s))] $$
策略梯度为：
$$ \nabla_\theta J(\mu_\theta) = \int_{\mathcal{S}} \rho^\mu(s) \nabla_\theta \mu_\theta(s) \nabla_a Q^\mu(s,a)|_{a=\mu_\theta(s)} ds = \mathbb{E}_{s\sim\rho^\mu}\left[ \nabla_\theta \mu_\theta(s) \nabla_a Q^\mu(s,a)|_{a=\mu_\theta(s)} \right] $$

**由于确定性策略不涉及对动作的积分，所以可以使用off-policy时不需要进行重要性采样的计算**

***Critic***

使用类似DQN的方式对**Q(s,a)**进行估计，不过a由确定性策略直接产生。这里也不需要重要性采样。


## 2019-01-10 WEEK-2
**工作要点**
1. DDPG代码
2. TD3(Twin Delayed DDPG)算法、代码
3. 初步阅读SAC(Soft Actor Critic)算法
4. 学习相关的一些知识

**DDPG算法小结**

Actor和Critic两个部分，都包含两个网络（Value网络和Target网络），对应论文中的u,u',Q,Q', 对应代码中的'main'和'targ'两个变量空间。
每次Value网络更新后的参数都会更新到Target网络参数上。

***stop_gradient使用***

计算yi的时候，常规做法是用一个placeholder来保存r+Q'(s',u'(s'))的值。而这里则使用targ网络，输入s',输出Q'(s',u'(s'))，但是使用stop_gradient将梯度回传截断，相当于产生了一个常量。

***网络参数共享***
[参考](https://www.cnblogs.com/MY0213/p/9208503.html)

在'main'里('targ'类似)，包含两个网络，一个是pi=u(s)产生策略输出，一个是q=Q(s,a)估计Q函数。而算法中对Q这个网络有两种使用需求，一个即a的来源，一个是采样(包含噪声)得到的a，视为已知常量a_ph;一个是u(s)生成的pi,即连接到u网络的输出上。

所以在代码中实现时使用
```with tf.variable_scope('q', reuse=True):```
即在这两个网络上共享一套参数。


**TD3 算法小结**

TD3目的是解决DDPG存在的对Q函数估值过高导致的策略失效问题。通过3个技巧进行改进：

1. 使用两个Q函数估计网络，在计算yi时取Q值较小的那个。在策略梯度中只固定使用Q1。

2. 降低policy更新频率，论文推荐为Q函数更新频率的一半。

3. target policy 平滑，即对a值、噪声\epsilon 都进行饱和处理。
