# Summary
## January
### 2019/01/02 WEEK-1

**工作要点**
1. 基于TF相对完整的实现了比较简单的DQN算法。
2. 总结并应用了相对清晰的设计流程。
3. experience replay部分存在问题。

<!-- **DQN算法理解** -->
<!-- 这部分待放入详细记录中。 -->

**神经网络结构设计流程**
1. 先确定神经网络的数据维度，使用tf.placeholder()进行指定。通常每一行为一个样本数据，所以维度一般是(?,n)，n为特征维度。
2. 自定义建立所使用的神经网络结构，这里主要采用了mlp()函数，参考spinning up相关代码。
3. 确定损失函数的形式。即根据RL算法中的梯度、更新形式，设计好相应的损失函数。
4. 确定好数据的获取、存储及神经网络的训练模式。
    - 获取为最基本的交互形式：o, r, d, _ = env.step(act)
    - 需要保存[o, a, r, o_]
    - 可以按照MC的形式，固定模型参数并采集一组episode 或是 完整的epoch 后进行训练
    - 也可以按照论文中experience replay的形式，每步迭代时都从存储空间中随机抽取样本组成minibatch并训练。**但是目前效果似乎有问题**
  
  
**DQN 损失函数的设计**

DQN的损失函数的形式为：
$$L_i(\theta_i) = \mathbb{E}{[y_i - Q(s,a;\theta_i)]^2}$$
其中，yi是样本标签，根据输入状态且固定网络参数后计算得到。
$$ y_i = r + \gamma \arg\max_{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i)$$

Q(s,a)则是每一步指定的(s,a)对应的值函数的值。但是由于还不知道怎么在placeholder不确定行数的情况下抽取每一行的指定列，所以这里计算的方法为：
1. 先计算每一个s对应的所有q_a(s)
2. 将q_a(s)赋值给target,然后根据记录的数据，将target中每一行对应使用a的那一列，改为y_i右边的前两项。


**算法实现流程**
1. 先在jupter里面进行设计，方便追踪和调试
2. 最开始的版本应该是特例版本：环境、参数、网络形式等。
3. 跑通后再对结构进行优化
4. 应该添加模型参数保存和读取部分，避免重复训练。




**ToDo List**
- [ ] 调试experience replay部分
- [X] 看看DQN在 Nature上的论文 变化了什么地方

## 2019-01-03 WEEK-1
**工作要点**
1. 粗读了DQN在Nature上的论文
2. 重新更新了JDAI课程的SVM作业

**Nature上的 DQN 升级版**
1. experience play部分，提到了可以不按照随机抽样的方式，而是可以考虑抽取能学习最多知识的样本，类似prioritized sweeping。但是文章中没有给出明确的实现。
2. 强调使用分离网络，即采用两个相同结构的NN，一个固定网络参数用于计算yi，另一个则从该参数开始迭代更新。当经过一定steps之后，才将新的参数赋值到第一个网络上。
3. 提到将误差项修剪到-1～1之间后能够提高网络的稳定性，但是也没有给出实现方法。

**SVM作业心得**
1. 将训练集的原始图像转变为二值图。
   - 大多数图片可以直接采用otsu，但是有些图片上下曝光量不一样，这个时候会出现较大误差。对于这些图片，可以将原图片分块进行otsu，最后拼接到一起。
   - 但是大多数图片如果也采用分块的方式，结果并不理想，**尤其是分块是纯色的情况**。
   - 目前还没有比较好的统一的处理办法，仍然是将曝光不同的图片手动调整。**猜想**可以采用权重的方式，将两种方法进行组合。
2. 特征组合
   - 单一特征无法实现100%正确率，错误分类的数量如下，随机选取300/500个样本训练无差别
   - 采用特征组合的方式，并且给不同的特征设计权重，特征1权重5,特征3权重1,在500个训练样本下可以做到100%正确分类。

    |特征编号|1|2|3|4|5|6|
    |:---:|:---:|:---:|:---:|:---:|:---:|:---:|
    |错分类数|195|9|1|853|23|65|


## 2019-01-04 WEEK-1
**工作要点**
1. 修改resume，搜索JD


## 2019-01-07 WEEK-2
**工作要点**
1. 学习TRPO代码

**TRPO代码小结**
1. 仍然是在Actor-Critic框架下进行，值函数估计的过程与VPG相同，Policy学习的过程中，不再是直接对目标函数进行简单的梯度下降优化求解了。
2. 



**其他几点思考**
1. 为什么Q-Learning不需要进行重要性采样？
2. VPG在Policy更新过程中，为什么也没有进行重要性采样？
